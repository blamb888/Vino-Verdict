{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../cleaned_wine_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>87</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description  points  price  \\\n",
       "0     Italy  Aromas include tropical fruit, broom, brimston...      87   19.0   \n",
       "1  Portugal  This is ripe and fruity, a wine that is smooth...      87   15.0   \n",
       "2        US  Tart and snappy, the flavors of lime flesh and...      87   14.0   \n",
       "\n",
       "            province           region_1  \\\n",
       "0  Sicily & Sardinia               Etna   \n",
       "1              Douro                NaN   \n",
       "2             Oregon  Willamette Valley   \n",
       "\n",
       "                                           title         variety  \\\n",
       "0              Nicosia 2013 Vulkà Bianco  (Etna)     White Blend   \n",
       "1  Quinta dos Avidagos 2011 Avidagos Red (Douro)  Portuguese Red   \n",
       "2  Rainstorm 2013 Pinot Gris (Willamette Valley)      Pinot Gris   \n",
       "\n",
       "                winery  year  \n",
       "0              Nicosia  2013  \n",
       "1  Quinta dos Avidagos  2011  \n",
       "2            Rainstorm  2013  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preprocess the `description` column\n",
    "\n",
    "# Check for missing values in the description and points columns\n",
    "missing_values = df[['description', 'points']].isnull().sum()\n",
    "\n",
    "# Drop rows with missing descriptions (if any)\n",
    "df = df.dropna(subset=['description'])\n",
    "\n",
    "# 2. Transform the `points` column into categorical labels\n",
    "\n",
    "# Define bins for the wine ratings and labels for each bin\n",
    "bins = [0, 85, 90, 100]\n",
    "labels = ['low', 'medium', 'high']\n",
    "\n",
    "# Create a new column 'rating_category' with the binned labels\n",
    "df['rating_category'] = pd.cut(df['points'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "missing_values, df[['description', 'rating_category']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the descriptions\n",
    "encoded_data = tokenizer.batch_encode_plus(\n",
    "    df['description'].values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    truncation=True,           # Explicitly truncate sequences exceeding max_length\n",
    "    padding='max_length',      # Pad all sequences to max_length\n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Extract the input IDs, attention masks, and labels\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = df['rating_category'].astype('category').cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert data into torch tensors\n",
    "input_ids = input_ids.clone().detach()\n",
    "attention_masks = attention_masks.clone().detach()\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create a tensor dataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Lists to store results for each fold\n",
    "validation_results = []\n",
    "\n",
    "# Define early stopping parameters\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "for train_index, val_index in kf.split(dataset):\n",
    "    \n",
    "    # Re-initialize the model for each fold\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", \n",
    "        num_labels=len(df['rating_category'].cat.categories),  \n",
    "        output_attentions=False, \n",
    "        output_hidden_states=False,\n",
    "        problem_type='single_label_classification'\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = TorchAdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=len(train_dataloader) * epochs)\n",
    "    \n",
    "    # Split the dataset into training and validation sets for this fold\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_index)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "    validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "    \n",
    "    best_val_f1_for_fold = 0  # To track the best F1 for this fold (reset for each fold)\n",
    "    epochs_without_improvement = 0  # To keep track of epochs without improvement\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels': batch[2]}\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_function(outputs.logits, inputs['labels'])\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights and learning rate\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        predictions, true_vals = [], []\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=batch[0], attention_mask=batch[1])\n",
    "            \n",
    "            logits = outputs[0].detach().cpu().numpy()\n",
    "            label_ids = batch[2].cpu().numpy()\n",
    "            predictions.extend(np.argmax(logits, axis=1))\n",
    "            true_vals.extend(label_ids)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(true_vals, predictions)\n",
    "        f1 = f1_score(true_vals, predictions, average='weighted')\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Check for F1 improvement\n",
    "        if f1 > best_val_f1_for_fold:\n",
    "            best_val_f1_for_fold = f1\n",
    "            epochs_without_improvement = 0  # Reset the counter\n",
    "            torch.save(model.state_dict(), f\"best_model_fold_{len(validation_results) + 1}.bin\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping after {epoch + 1} epochs!\")\n",
    "            break\n",
    "        \n",
    "        # Optionally clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    validation_results.append({'Accuracy': accuracy, 'F1': f1})\n",
    "\n",
    "# Analyze validation_results after all folds are completed\n",
    "print(validation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vino_verdict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
