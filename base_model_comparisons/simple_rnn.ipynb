{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../cleaned_wine_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>87</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description  points  price  \\\n",
       "0     Italy  Aromas include tropical fruit, broom, brimston...      87   19.0   \n",
       "1  Portugal  This is ripe and fruity, a wine that is smooth...      87   15.0   \n",
       "2        US  Tart and snappy, the flavors of lime flesh and...      87   14.0   \n",
       "\n",
       "            province           region_1  \\\n",
       "0  Sicily & Sardinia               Etna   \n",
       "1              Douro                NaN   \n",
       "2             Oregon  Willamette Valley   \n",
       "\n",
       "                                           title         variety  \\\n",
       "0              Nicosia 2013 Vulkà Bianco  (Etna)     White Blend   \n",
       "1  Quinta dos Avidagos 2011 Avidagos Red (Douro)  Portuguese Red   \n",
       "2  Rainstorm 2013 Pinot Gris (Willamette Valley)      Pinot Gris   \n",
       "\n",
       "                winery  year  \n",
       "0              Nicosia  2013  \n",
       "1  Quinta dos Avidagos  2011  \n",
       "2            Rainstorm  2013  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'description', 'points', 'price', 'province', 'region_1',\n",
       "       'title', 'variety', 'winery', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the data into training and testing sets small enough to fit into memory, also adjust to a 5 point scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# Function to adjust the scale of 'points' data\n",
    "def points_to_scale(points, scale=5):\n",
    "    points_norm = (points - np.min(points)) / (np.max(points) - np.min(points))\n",
    "    return np.round(points_norm * scale + 1).astype(int)\n",
    "\n",
    "# Apply transformation to points\n",
    "df['points'] = points_to_scale(df['points'])\n",
    "\n",
    "# Function to load data\n",
    "def load_data(df, percentage_of_data=None):\n",
    "    sentences = df['description']\n",
    "    y = df['points']\n",
    "    \n",
    "    if percentage_of_data is not None:\n",
    "        assert(percentage_of_data > 0 and percentage_of_data <= 100)\n",
    "        len_data = int(percentage_of_data / 100 * len(sentences))\n",
    "        sentences, y = sentences[:len_data], y[:len_data]\n",
    "    \n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=42)\n",
    "    X_train = [text_to_word_sequence(text) for text in sentences_train]\n",
    "    X_test = [text_to_word_sequence(text) for text in sentences_test]\n",
    "\n",
    "    return X_train, y_train.to_numpy(), X_test, y_test.to_numpy()\n",
    "\n",
    "# Call the load_data function to split and preprocess data\n",
    "X_train, y_train, X_test, y_test = load_data(df, percentage_of_data=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a word2vec model on the sample corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a word2vec model on the training data with the following parameters:\n",
    "# - size: 100\n",
    "# - window: 5\n",
    "# - min_count: 3\n",
    "\n",
    "word2vec = Word2Vec(X_train, vector_size=100, window=5, min_count=3)\n",
    "wv = word2vec.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Convert Training Data into something we can feed into an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test that X_train and X_test are numpy arrays with shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ME\n",
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "It is always good to have a very simple model to test your own model against - to be sure you are doing something better than a very simple algorithm.\n",
    "\n",
    "❓ **Question** ❓ What is your baseline accuracy? In this case, your baseline can be to predict the label that is the most present in `y_train` (of course, if the dataset is balanced, the baseline accuracy is 1/n where n is the number of classes - 2 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.33780276816608995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the most frequent label\n",
    "most_frequent_label = np.argmax(np.bincount(y_train))\n",
    "\n",
    "# Calculate the baseline accuracy\n",
    "baseline_accuracy = np.mean(y_test == most_frequent_label)\n",
    "\n",
    "print(\"Baseline Accuracy:\", baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The BASIC model with no transformer power    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense\n",
    "\n",
    "def build_rnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0, input_shape=input_shape))\n",
    "    model.add(LSTM(20, activation='tanh'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Check if the model is above the baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 20:21:56.923756: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 591840000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "232/232 [==============================] - 24s 90ms/step - loss: 1.3368 - accuracy: 0.3901 - val_loss: 1.1494 - val_accuracy: 0.4319\n",
      "Epoch 2/20\n",
      "232/232 [==============================] - 21s 89ms/step - loss: 1.0899 - accuracy: 0.4880 - val_loss: 1.1392 - val_accuracy: 0.4616\n",
      "Epoch 3/20\n",
      "232/232 [==============================] - 23s 100ms/step - loss: 1.0323 - accuracy: 0.5249 - val_loss: 1.1267 - val_accuracy: 0.4638\n",
      "Epoch 4/20\n",
      "232/232 [==============================] - 23s 97ms/step - loss: 0.9892 - accuracy: 0.5522 - val_loss: 1.0545 - val_accuracy: 0.5124\n",
      "Epoch 5/20\n",
      "232/232 [==============================] - 21s 92ms/step - loss: 0.9701 - accuracy: 0.5633 - val_loss: 0.9836 - val_accuracy: 0.5573\n",
      "Epoch 6/20\n",
      "232/232 [==============================] - 23s 101ms/step - loss: 0.9518 - accuracy: 0.5681 - val_loss: 1.0145 - val_accuracy: 0.5292\n",
      "Epoch 7/20\n",
      "232/232 [==============================] - 23s 100ms/step - loss: 0.9376 - accuracy: 0.5729 - val_loss: 0.9616 - val_accuracy: 0.5514\n",
      "Epoch 8/20\n",
      "232/232 [==============================] - 23s 98ms/step - loss: 0.9251 - accuracy: 0.5804 - val_loss: 0.9560 - val_accuracy: 0.5714\n",
      "Epoch 9/20\n",
      "232/232 [==============================] - 24s 102ms/step - loss: 0.9111 - accuracy: 0.5934 - val_loss: 1.1044 - val_accuracy: 0.5124\n",
      "Epoch 10/20\n",
      "232/232 [==============================] - 23s 101ms/step - loss: 0.9072 - accuracy: 0.5938 - val_loss: 1.2267 - val_accuracy: 0.4362\n",
      "Epoch 11/20\n",
      "232/232 [==============================] - 23s 98ms/step - loss: 0.8986 - accuracy: 0.5954 - val_loss: 0.9701 - val_accuracy: 0.5584\n",
      "Epoch 12/20\n",
      "232/232 [==============================] - 23s 99ms/step - loss: 0.8891 - accuracy: 0.6022 - val_loss: 0.9995 - val_accuracy: 0.5519\n",
      "Epoch 13/20\n",
      "232/232 [==============================] - 24s 103ms/step - loss: 0.8839 - accuracy: 0.6065 - val_loss: 0.9839 - val_accuracy: 0.5589\n"
     ]
    }
   ],
   "source": [
    "# Find the number of unique classes\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "# Ensure that classes start from 0\n",
    "y_train -= y_train.min()\n",
    "y_test -= y_test.min()\n",
    "\n",
    "# Instantiate the model\n",
    "model = build_rnn_model(input_shape=X_train_pad.shape[1:], num_classes=num_classes)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_oh = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_oh = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Define Early Stopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train_pad, y_train_oh, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 2s 26ms/step - loss: 0.9575 - accuracy: 0.5597\n",
      "Test Accuracy: 0.5596885681152344\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_oh)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vino_verdict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
