{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../cleaned_wine_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>87</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description  points  price  \\\n",
       "0     Italy  Aromas include tropical fruit, broom, brimston...      87   19.0   \n",
       "1  Portugal  This is ripe and fruity, a wine that is smooth...      87   15.0   \n",
       "2        US  Tart and snappy, the flavors of lime flesh and...      87   14.0   \n",
       "\n",
       "            province           region_1  \\\n",
       "0  Sicily & Sardinia               Etna   \n",
       "1              Douro                NaN   \n",
       "2             Oregon  Willamette Valley   \n",
       "\n",
       "                                           title         variety  \\\n",
       "0              Nicosia 2013 Vulkà Bianco  (Etna)     White Blend   \n",
       "1  Quinta dos Avidagos 2011 Avidagos Red (Douro)  Portuguese Red   \n",
       "2  Rainstorm 2013 Pinot Gris (Willamette Valley)      Pinot Gris   \n",
       "\n",
       "                winery  year  \n",
       "0              Nicosia  2013  \n",
       "1  Quinta dos Avidagos  2011  \n",
       "2            Rainstorm  2013  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'description', 'points', 'price', 'province', 'region_1',\n",
       "       'title', 'variety', 'winery', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the data into training and testing sets small enough to fit into memory, also adjust to a 5 point scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 14:46:35.424696: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-05 14:46:35.459963: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-05 14:46:35.461228: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-05 14:46:36.807441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# Function to adjust the scale of 'points' data\n",
    "def points_to_scale(points, scale=5):\n",
    "    points_norm = (points - np.min(points)) / (np.max(points) - np.min(points))\n",
    "    return np.round(points_norm * scale + 1).astype(int)\n",
    "\n",
    "# Apply transformation to points\n",
    "df['points'] = points_to_scale(df['points'])\n",
    "\n",
    "# Function to load data\n",
    "def load_data(df, percentage_of_data=None):\n",
    "    sentences = df['description']\n",
    "    y = df['points']\n",
    "    \n",
    "    if percentage_of_data is not None:\n",
    "        assert(percentage_of_data > 0 and percentage_of_data <= 100)\n",
    "        len_data = int(percentage_of_data / 100 * len(sentences))\n",
    "        sentences, y = sentences[:len_data], y[:len_data]\n",
    "    \n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=42)\n",
    "    X_train = [text_to_word_sequence(text) for text in sentences_train]\n",
    "    X_test = [text_to_word_sequence(text) for text in sentences_test]\n",
    "\n",
    "    return X_train, y_train.to_numpy(), X_test, y_test.to_numpy()\n",
    "\n",
    "# Call the load_data function to split and preprocess data\n",
    "X_train, y_train, X_test, y_test = load_data(df, percentage_of_data=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a word2vec model on the sample corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a word2vec model on the training data with the following parameters:\n",
    "# - size: 100\n",
    "# - window: 5\n",
    "# - min_count: 3\n",
    "\n",
    "word2vec = Word2Vec(X_train, vector_size=100, window=5, min_count=3)\n",
    "wv = word2vec.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Convert Training Data into something we can feed into an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test that X_train and X_test are numpy arrays with shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ME\n",
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "It is always good to have a very simple model to test your own model against - to be sure you are doing something better than a very simple algorithm.\n",
    "\n",
    "❓ **Question** ❓ What is your baseline accuracy? In this case, your baseline can be to predict the label that is the most present in `y_train` (of course, if the dataset is balanced, the baseline accuracy is 1/n where n is the number of classes - 2 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.33780276816608995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the most frequent label\n",
    "most_frequent_label = np.argmax(np.bincount(y_train))\n",
    "\n",
    "# Calculate the baseline accuracy\n",
    "baseline_accuracy = np.mean(y_test == most_frequent_label)\n",
    "\n",
    "print(\"Baseline Accuracy:\", baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The BASIC model with no transformer power    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense\n",
    "\n",
    "def build_rnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0, input_shape=input_shape))\n",
    "    model.add(LSTM(20, activation='tanh'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Check if the model is above the baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 14:58:06.751070: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 591840000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - ETA: 0s - loss: 1.3502 - accuracy: 0.3663"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 14:58:41.414186: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 148000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 27s 97ms/step - loss: 1.3502 - accuracy: 0.3663 - val_loss: 1.1915 - val_accuracy: 0.3724\n",
      "Epoch 2/20\n",
      "232/232 [==============================] - 22s 95ms/step - loss: 1.1166 - accuracy: 0.4747 - val_loss: 1.1581 - val_accuracy: 0.4449\n",
      "Epoch 3/20\n",
      "232/232 [==============================] - 23s 98ms/step - loss: 1.0412 - accuracy: 0.5356 - val_loss: 1.0315 - val_accuracy: 0.5292\n",
      "Epoch 4/20\n",
      "232/232 [==============================] - 23s 98ms/step - loss: 0.9977 - accuracy: 0.5522 - val_loss: 1.0942 - val_accuracy: 0.4935\n",
      "Epoch 5/20\n",
      "232/232 [==============================] - 24s 102ms/step - loss: 0.9724 - accuracy: 0.5585 - val_loss: 0.9940 - val_accuracy: 0.5486\n",
      "Epoch 6/20\n",
      "232/232 [==============================] - 24s 104ms/step - loss: 0.9497 - accuracy: 0.5662 - val_loss: 1.1212 - val_accuracy: 0.4676\n",
      "Epoch 7/20\n",
      "232/232 [==============================] - 25s 107ms/step - loss: 0.9379 - accuracy: 0.5761 - val_loss: 1.0272 - val_accuracy: 0.5292\n",
      "Epoch 8/20\n",
      "232/232 [==============================] - 24s 103ms/step - loss: 0.9203 - accuracy: 0.5850 - val_loss: 0.9613 - val_accuracy: 0.5519\n",
      "Epoch 9/20\n",
      "232/232 [==============================] - 25s 106ms/step - loss: 0.9092 - accuracy: 0.5899 - val_loss: 0.9767 - val_accuracy: 0.5503\n",
      "Epoch 10/20\n",
      "232/232 [==============================] - 29s 124ms/step - loss: 0.8965 - accuracy: 0.5960 - val_loss: 0.9433 - val_accuracy: 0.5762\n",
      "Epoch 11/20\n",
      "232/232 [==============================] - 33s 142ms/step - loss: 0.8872 - accuracy: 0.5977 - val_loss: 0.9295 - val_accuracy: 0.5870\n",
      "Epoch 12/20\n",
      "232/232 [==============================] - 34s 146ms/step - loss: 0.8810 - accuracy: 0.6061 - val_loss: 0.9588 - val_accuracy: 0.5627\n",
      "Epoch 13/20\n",
      "232/232 [==============================] - 33s 141ms/step - loss: 0.8744 - accuracy: 0.6080 - val_loss: 0.9312 - val_accuracy: 0.5805\n",
      "Epoch 14/20\n",
      "232/232 [==============================] - 33s 142ms/step - loss: 0.8641 - accuracy: 0.6084 - val_loss: 0.9754 - val_accuracy: 0.5454\n",
      "Epoch 15/20\n",
      "232/232 [==============================] - 33s 142ms/step - loss: 0.8551 - accuracy: 0.6171 - val_loss: 0.9519 - val_accuracy: 0.5632\n",
      "Epoch 16/20\n",
      "232/232 [==============================] - 33s 143ms/step - loss: 0.8492 - accuracy: 0.6222 - val_loss: 0.9172 - val_accuracy: 0.5968\n",
      "Epoch 17/20\n",
      "232/232 [==============================] - 33s 143ms/step - loss: 0.8401 - accuracy: 0.6219 - val_loss: 0.9352 - val_accuracy: 0.5670\n",
      "Epoch 18/20\n",
      "232/232 [==============================] - 35s 152ms/step - loss: 0.8338 - accuracy: 0.6260 - val_loss: 1.0558 - val_accuracy: 0.5314\n",
      "Epoch 19/20\n",
      "232/232 [==============================] - 33s 142ms/step - loss: 0.8267 - accuracy: 0.6326 - val_loss: 0.9368 - val_accuracy: 0.5773\n",
      "Epoch 20/20\n",
      "232/232 [==============================] - 33s 142ms/step - loss: 0.8259 - accuracy: 0.6302 - val_loss: 1.1195 - val_accuracy: 0.5081\n"
     ]
    }
   ],
   "source": [
    "# Find the number of unique classes\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "# Ensure that classes start from 0\n",
    "y_train -= y_train.min()\n",
    "y_test -= y_test.min()\n",
    "\n",
    "# Instantiate the model\n",
    "model = build_rnn_model(input_shape=X_train_pad.shape[1:], num_classes=num_classes)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_oh = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_oh = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Define Early Stopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train_pad, y_train_oh, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/73 [>.............................] - ETA: 1s - loss: 1.1486 - accuracy: 0.4688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 15:08:07.154057: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 184960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 2s 25ms/step - loss: 1.1071 - accuracy: 0.5177\n",
      "Test Accuracy: 0.5177335739135742\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_oh)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vino_verdict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
